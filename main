import os
import pandas as pd
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers
import tensorflow as tf
import math
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam


# Load files
folderPath = "C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data"
filenameListFile = 'participant_demog.csv'

# Read the file with filenames
filenameTable = pd.read_csv(os.path.join(folderPath, filenameListFile))

# Extract the filenames from the first column
fileNames = filenameTable.iloc[:, 0].values
'''
numFiles = len(fileNames)  # Get the number of files

A = np.zeros((numFiles, 3))  # Initialize the result matrix

for i in range(numFiles):
    filePath = os.path.join(folderPath, fileNames[i])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Perform calculations on the loaded data
    Array = data.values
    Walking = np.sum(Array[:, 0] == 1)
    Stairs_up = np.sum(Array[:, 0] == 3)
    Stairs_down = np.sum(Array[:, 0] == 2)

    A[i, :] = [Walking, Stairs_up, Stairs_down]

# Calculate the sum of each column of A
sum_A = np.sum(A, axis=0)

# Plot the results
X = ['Walking', 'Stairs up', 'Stairs down']
fig, ax = plt.subplots()
ax.bar(X, sum_A, linewidth=1.0)
plt.show()


# Initialize arrays to store time and action values
time = [None] * numFiles
actions = [None] * numFiles

for i in range(numFiles):
    filePath = os.path.join(folderPath, fileNames[i])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Extract time and action columns from the data
    time[i] = data['time_s']
    actions[i] = data['activity']

    # Plot the actions versus time for the current file
    plt.figure()
    plt.scatter(time[i], actions[i], marker='.', color='k')
    plt.title('Actions vs. Time - File: ' + fileNames[i])
    plt.legend(['1 - Walking', '2 - Stairs down', '3 - Stairs up'])
    plt.ylim([0, 3])
    plt.xlabel('Time')
    plt.ylabel('Action')

plt.show()


# Define the columns to extract
columns = ['lw', 'lh', 'la', 'ra']
axis_labels = ['x', 'y', 'z']

# Create the subplots
fig, axs = plt.subplots(2, 2)

for i in range(3):
    filePath = os.path.join(folderPath, fileNames[i])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Extract time column from the data
    time = data['time_s']

    # Iterate over the desired columns and create the scatter plots
    for j in range(3):
        axs[j // 2, j % 2].scatter(time, data[columns[j] + '_' + axis_labels[i]], marker='.')
        axs[j // 2, j % 2].set_xlabel('Time')
        axs[j // 2, j % 2].set_ylabel(columns[j] + ' Axis')

plt.show()


# Getting rid of the data not used in the classifier
folderPath = 'C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data'

# Iterate over each CSV file
for file_index in range(1, 33):
    filePath = os.path.join(folderPath, fileNames[file_index - 1])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Filter out rows with labels 4, 77, and 99
    data = data[~data['activity'].isin([4, 77, 99])]

    # Save the modified data to a new CSV file
    filename = filePath.split('\\')[-1]  # Extract the filename from the full path
    modified_csv_path = 'C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data\\modified\\' + filename + '.csv'
    data.to_csv(modified_csv_path, index=False)

print("Modified CSV files saved successfully.")
'''

########################################################################################################################

# Normalization
normalized_tables = []

modified_csv_path_1 = 'C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data\\modified\\'
for file_index in range(1, 33):
    filePath = os.path.join(modified_csv_path_1, fileNames[file_index - 1])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Create a new DataFrame to store the normalized columns
    normalized_table = pd.DataFrame()

    # Normalize each column in the table
    column_names = data.columns
    for col_index in range(2, len(column_names)):
        col_name = column_names[col_index]
        normalized_col_name = 'Normalized' + col_name
        normalized_table[normalized_col_name] = (data[col_name] - data[col_name].mean()) / data[col_name].std()

    # Store the normalized table in the list
    normalized_tables.append(normalized_table)

# Framing
framed_data = []

# Define the frame size and overlap
frame_size = 100  # Specify the frame size (number of rows per frame)
overlap = 50  # Specify the overlap (number of overlapping rows between frames)

for file_index in range(32):
    # Perform framing on the normalized table
    normalized_table = normalized_tables[file_index]
    num_frames = int((len(normalized_table) - frame_size) / overlap) + 1
    frames = []
    for frame_index in range(num_frames):
        start_row = frame_index * overlap
        end_row = start_row + frame_size
        frames.append(normalized_table.iloc[start_row:end_row])

    # Store the framed data in the list
    framed_data.append(frames)

# Train and test data
num_classes = 3  # Specify the number of classes in your dataset

# Concatenate all the framed data into a single array
data = np.concatenate(framed_data, axis=0)
num_frames = data.shape[0]
labels = np.tile(np.arange(1, num_classes + 1), num_frames // num_classes)

num_frames = min(num_frames, len(labels))

# Randomly shuffle the data
idx = np.random.permutation(num_frames)
shuffled_data = data[idx]
shuffled_labels = labels[idx][:num_frames]

# Trim the shuffled data and labels arrays to the valid indices
shuffled_data = shuffled_data[:num_frames]
shuffled_labels = shuffled_labels[:num_frames]

# Split the data into train and test sets
train_ratio = 0.8  # Specify the ratio of training data
num_train = round(train_ratio * num_frames)
train_data = shuffled_data[:num_train]
train_labels = shuffled_labels[:num_train]
test_data = shuffled_data[num_train:]
test_labels = shuffled_labels[num_train:]

# Adjust the labels to start from 0 instead of 1
train_labels -= 1
test_labels -= 1

# Convert the labels to one-hot encoded format
train_labels = to_categorical(train_labels, num_classes=num_classes)
test_labels = to_categorical(test_labels, num_classes=num_classes)

########################################################################################################################

# Reshape the data for CNN input
train_data = train_data.reshape(-1, frame_size, len(column_names) - 2, 1)
test_data = test_data.reshape(-1, frame_size, len(column_names) - 2, 1)

# Define the CNN model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(frame_size, len(column_names) - 2, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

# Set a lower learning rate
learning_rate = 0.00001

# Compile the model with the Adam optimizer and lower learning rate
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(train_data, train_labels, epochs=20, batch_size=32)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(test_data, test_labels)
print('Test Accuracy:', test_accuracy)

# Predict labels using the trained model
predicted_labels = np.argmax(model.predict(test_data), axis=1)

# Convert one-hot encoded true labels back to categorical labels
true_labels = np.argmax(test_labels, axis=1)


# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()
