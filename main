import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Load files
folderPath = "C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data"  # Specify the folder path

filenameListFile = 'participant_demog.csv'  # Name of the file containing the filenames

# Read the file with filenames
filenameTable = pd.read_csv(os.path.join(folderPath, filenameListFile))

# Extract the filenames from the first column
fileNames = filenameTable.iloc[:, 0].values

numFiles = len(fileNames)  # Get the number of files

A = np.zeros((numFiles, 3))  # Initialize the result matrix
'''
for i in range(numFiles):
    filePath = os.path.join(folderPath, fileNames[i])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Perform calculations on the loaded data
    Array = data.values
    Walking = np.sum(Array[:, 0] == 1)
    Stairs_up = np.sum(Array[:, 0] == 3)
    Stairs_down = np.sum(Array[:, 0] == 2)

    A[i, :] = [Walking, Stairs_up, Stairs_down]

# Calculate the sum of each column of A
sum_A = np.sum(A, axis=0)





# Plot the results
X = ['Walking', 'Stairs up', 'Stairs down']
fig, ax = plt.subplots()
ax.bar(X, sum_A, linewidth=1.0)
plt.show()
plt.close() ########################################################


# Initialize arrays to store time and action values
time = [None] * numFiles
actions = [None] * numFiles

for i in range(numFiles):
    filePath = os.path.join(folderPath, fileNames[i])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Extract time and action columns from the data
    time[i] = data['time_s']
    actions[i] = data['activity']

    # Plot the actions versus time for the current file
    plt.figure()
    plt.scatter(time[i], actions[i], marker='.', color='k')
    plt.title('Actions vs. Time - File: ' + fileNames[i])
    plt.legend(['1 - Walking', '2 - Stairs down', '3 - Stairs up'])
    plt.ylim([0, 3])
    plt.xlabel('Time')
    plt.ylabel('Action')

plt.show()
plt.close()##################################################################


# Define the columns to extract
columns = ['lw', 'lh', 'la', 'ra']
axis_labels = ['x', 'y', 'z']

# Create the subplots
fig, axs = plt.subplots(2, 2)

for i in range(3):
    filePath = os.path.join(folderPath, fileNames[i])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Extract time column from the data
    time = data['time_s']

    # Iterate over the desired columns and create the scatter plots
    for j in range(3):
        axs[j // 2, j % 2].scatter(time, data[columns[j] + '_' + axis_labels[i]], marker='.')
        axs[j // 2, j % 2].set_xlabel('Time')
        axs[j // 2, j % 2].set_ylabel(columns[j] + ' Axis')

plt.show()
plt.close()#################################################################
'''

######################################################################################
#Getting rid of the data not used in the classifier
'''
import os
import pandas as pd

folderPath = 'C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data'

# Iterate over each CSV file
for file_index in range(1, 33):
    filePath = os.path.join(folderPath, fileNames[file_index - 1])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Filter out rows with labels 4, 5, and 99
    data = data[~data['activity'].isin([4, 77, 99])]

    # Save the modified data to a new CSV file
    filename = filePath.split('\\')[-1]  # Extract the filename from the full path
    modified_csv_path = 'C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data\\modified\\' + filename + '.csv'
    data.to_csv(modified_csv_path, index=False)

print("Modified CSV files saved successfully.")
'''

############################################################################################################################
# Normalization
normalized_tables = []

modified_csv_path_1 = 'C:\\Users\\matla\\PycharmProjects\\pythonProject\\raw-accelerometry-data\\modified\\'
for file_index in range(1, 33):
    filePath = os.path.join(modified_csv_path_1, fileNames[file_index - 1])  # Create the full file path

    # Load the data from the file
    data = pd.read_csv(filePath + '.csv')

    # Create a new DataFrame to store the normalized columns
    normalized_table = pd.DataFrame()

    # Normalize each column in the table
    column_names = data.columns
    for col_index in range(2, len(column_names)):
        col_name = column_names[col_index]
        normalized_col_name = 'Normalized' + col_name
        normalized_table[normalized_col_name] = (data[col_name] - data[col_name].mean()) / data[col_name].std()

    # Store the normalized table in the list
    normalized_tables.append(normalized_table)

# Framing
framed_data = []

# Define the frame size and overlap
frame_size = 50  # Specify the frame size (number of rows per frame)
overlap = 25  # Specify the overlap (number of overlapping rows between frames)

for file_index in range(32):
    # Perform framing on the normalized table
    normalized_table = normalized_tables[file_index]
    num_frames = int((len(normalized_table) - frame_size) / overlap) + 1
    frames = []
    for frame_index in range(num_frames):
        start_row = frame_index * overlap
        end_row = start_row + frame_size
        frames.append(normalized_table.iloc[start_row:end_row])

    # Store the framed data in the list
    framed_data.append(frames)

# Train and test data
num_classes = 3  # Specify the number of classes in your dataset

# Concatenate all the framed data into a single array
data = np.concatenate(framed_data, axis=0)
num_frames = data.shape[0]
labels = np.tile(np.arange(1, num_classes + 1), num_frames // num_classes)

num_frames = min(num_frames, len(labels))


# Randomly shuffle the data
idx = np.random.permutation(num_frames)
shuffled_data = data[idx]
shuffled_labels = labels[idx][:num_frames]


# Trim the shuffled data and labels arrays to the valid indices
shuffled_data = shuffled_data[:num_frames]
shuffled_labels = shuffled_labels[:num_frames]



# Split the data into train and test sets
train_ratio = 0.8  # Specify the ratio of training data
num_train = round(train_ratio * num_frames)
train_data = shuffled_data[:num_train]
train_labels = shuffled_labels[:num_train]
test_data = shuffled_data[num_train:]
test_labels = shuffled_labels[num_train:]


#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################


import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers

# ... Rest of the code ...

import tensorflow as tf
import math

# Reshape the input data for CNN
input_shape = train_data[0].shape
train_data = train_data.reshape(-1, input_shape[0], input_shape[1], 1)
test_data = test_data.reshape(-1, input_shape[0], input_shape[1], 1)

# Convert labels to one-hot encoding
train_labels = tf.keras.utils.to_categorical(train_labels - 1, num_classes)
test_labels = tf.keras.utils.to_categorical(test_labels - 1, num_classes)

# Create a CNN model
model = tf.keras.Sequential([
    layers.Conv2D(256, (3, 3), activation='relu', input_shape=(input_shape[0], input_shape[1], 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(256, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

# Define a learning rate schedule using the step decay method
def step_decay(epoch):
    initial_learning_rate = 0.0001
    drop = 0.5
    epochs_drop = 10
    learning_rate = initial_learning_rate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    return learning_rate

# Create a learning rate scheduler callback
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)

# Example code using the SGD optimizer with an initial learning rate of 0.001
optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)

# Compile the model
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model with the learning rate scheduler callback
history = model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=10, batch_size=128, callbacks=[lr_scheduler])

# Evaluate the model
test_loss, test_accuracy = model.evaluate(test_data, test_labels)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# Predict labels using the trained model
predicted_labels = np.argmax(model.predict(test_data), axis=1)

# Convert one-hot encoded true labels back to categorical labels
true_labels = np.argmax(test_labels, axis=1)

# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()
